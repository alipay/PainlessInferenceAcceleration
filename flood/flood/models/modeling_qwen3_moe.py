# -*- coding: utf-8 -*-
# Copyright (c) Ant Financial Service Group and its affiliates.

# NOTE: the file is generated by scaffold.py

import os
import math
import time
import copy
from typing import List, Optional, Tuple, Union, Dict

import torch
from transformers.cache_utils import Cache
from transformers.modeling_utils import PreTrainedModel
from transformers.modeling_utils import PretrainedConfig


from flood.ops.activation import silu_and_mul
from flood.ops.norm import RMSNorm
from flood.utils.batch import Batch
from flood.layers.linear import AutoLinear
from flood.layers.rope import AutoRope
from flood.layers.attention import AutoAttention
from flood.layers.embedding import AutoEmbedding
from flood.layers.sampler import Sampler
from flood.layers.moe import AutoExperts
from transformers.models.qwen3_moe.configuration_qwen3_moe import Qwen3MoeConfig



class Qwen3MoeMLP(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size

        self.gate_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                    self.intermediate_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='gate_proj')
        self.up_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                  self.intermediate_size, 
                                                  bias=False, 
                                                  config=config, 
                                                  name='up_proj')
            
        self.down_proj = AutoLinear.from_pretrained(self.intermediate_size, 
                                                    self.hidden_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='down_proj')

    def _flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch MLP')

        self.gate_up_proj = self.gate_proj.merge([self.gate_proj, self.up_proj])
        self.down_proj.patch()

        self.gate_proj = None
        delattr(self, 'gate_proj')

        self.up_proj = None
        delattr(self, 'up_proj')
            
    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        act = silu_and_mul(gate_up)
        return self.down_proj(act)


class Qwen3MoeSparseMoeBlock(torch.nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        layer_idx: int = 0
    ):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.num_experts = config.num_experts
        self.top_k = config.num_experts_per_tok
        self.norm_topk_prob = self.config.norm_topk_prob
        exp_conf = copy.deepcopy(config)
        exp_conf.intermediate_size = config.moe_intermediate_size

        modules = torch.nn.ModuleList([Qwen3MoeMLP(exp_conf, layer_idx=-1)
                                        for _ in range(self.num_experts)])
        self.experts = AutoExperts.from_pretrained(module_list=modules,
                                                    hidden_size=exp_conf.hidden_size,
                                                    intermediate_size=exp_conf.intermediate_size,
                                                    num_expert=self.num_experts,
                                                    scoring_func='softmax',
                                                    config=config,
                                                    )

        self.gate = torch.nn.Linear(config.hidden_size,
                                     self.num_experts,
                                     bias=False)

    def flood_patch_func(self, kwargs=None):
        self.experts._flood_patch_func()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        num_tokens, hidden_dim = hidden_states.shape
        hidden_states = hidden_states.view(-1, hidden_dim)

        router_logits = self.gate(hidden_states)

        final_hidden_states = self.experts(hidden_states,
                                        router_logits,
                                        self.top_k,
                                        renormalize=self.norm_topk_prob,
                                        )
        return final_hidden_states.view(num_tokens, hidden_dim)



# class Qwen3MoeSparseMoeBlock(torch.nn.Module):
#     def __init__(self, config, layer_idx):
#         super().__init__()
#         self.num_experts = config.num_experts
#         self.top_k = config.num_experts_per_tok
#         self.norm_topk_prob = config.norm_topk_prob

#         # gating
#         self.gate = torch.nn.Linear(config.hidden_size, config.num_experts, bias=False)
#         self.experts = torch.nn.ModuleList(
#             [Qwen3MoeMLP(config, layer_idx=-1) for _ in range(self.num_experts)]
#         )

#     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
#         """ """
#         num_tokens, hidden_dim = hidden_states.shape
#         hidden_states = hidden_states.view(-1, hidden_dim)
#         # router_logits: (batch * sequence_length, n_experts)
#         router_logits = self.gate(hidden_states)

#         routing_weights = torch.nn.functional.softmax(router_logits, dim=1, dtype=torch.float)
#         routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
#         if self.norm_topk_prob:  # only diff with mixtral sparse moe block!
#             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
#         # we cast back to the input dtype
#         routing_weights = routing_weights.to(hidden_states.dtype)

#         final_hidden_states = torch.zeros(
#             (num_tokens, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device
#         )

#         # One hot encode the selected experts to create an expert mask
#         # this will be used to easily index which expert is going to be sollicitated
#         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)

#         # Loop over all available experts in the model and perform the computation on each expert
#         expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()
#         for expert_idx in expert_hitted:
#             expert_layer = self.experts[expert_idx]
#             idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))

#             # Index the correct hidden states and compute the expert hidden state for
#             # the current expert. We need to make sure to multiply the output hidden
#             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)
#             current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)
#             current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]

#             # However `index_add_` only support torch tensors for indexing so we'll use
#             # the `top_x` tensor here.
#             final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))
#         final_hidden_states = final_hidden_states.reshape(num_tokens, hidden_dim)
#         return final_hidden_states

class Qwen3MoeAttention(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        head_dim = None
        if hasattr(config, 'head_dim'):
            head_dim = config.head_dim
        if head_dim is None or head_dim<=0:
            head_dim = self.hidden_size // self.num_heads
        self.head_dim = head_dim
        self.intermediate_size = self.num_heads * self.head_dim
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = float(config.rope_theta)
        self.softmax_scale = math.sqrt(1.0 / self.head_dim)

        self.q_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_heads * self.head_dim, 
                                                 bias=config.attention_bias, 
                                                 config=config, 
                                                 name='q_proj')
        self.k_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_key_value_heads * self.head_dim, 
                                                 bias=config.attention_bias, 
                                                 config=config, 
                                                 name='k_proj')
        self.v_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_key_value_heads * self.head_dim, 
                                                 bias=config.attention_bias, 
                                                 config=config,
                                                 name='v_proj')
        self.o_proj = AutoLinear.from_pretrained(self.intermediate_size, 
                                                 self.hidden_size, 
                                                 bias=config.attention_bias, 
                                                 config=config, 
                                                 name='o_proj')
        self.q_norm = RMSNorm(self.head_dim, eps=config.rms_norm_eps)
        self.k_norm = RMSNorm(self.head_dim, eps=config.rms_norm_eps)

        self.rope = AutoRope.from_pretrained(config)
        self.attention =  None

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch Attention')

        self.qkv_proj = self.q_proj.merge([self.q_proj, self.k_proj, self.v_proj])

        self.o_proj.patch()

        self.q_proj = None
        delattr(self, 'q_proj')

        self.k_proj = None
        delattr(self, 'k_proj')

        self.v_proj = None
        delattr(self, 'v_proj')

        if kwargs is None:
            kwargs = {}
        cache_dtype = kwargs.get('cache_dtype', None)
        interleave_value = kwargs.get('interleave_value', False)
        if interleave_value:
            AutoAttention.interleave(self.qkv_proj, 
                                     self.num_heads, 
                                     self.num_key_value_heads, 
                                     self.head_dim)

        kernels = kwargs.get('kernels', ['sa'])
        self.attention = AutoAttention.from_pretrained(cache_dtype, 
                                                       layer_idx=self.layer_idx, 
                                                       kernels=kernels,
                                                       softmax_scale=self.softmax_scale)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        **kwargs,
    ) -> torch.Tensor:

        q_len = hidden_states.size(0)
        qkv = self.qkv_proj(hidden_states)
        qkv = qkv.view(q_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)

        query_states, key_states, value_states = qkv.split([self.num_heads, 
                                                            self.num_key_value_heads, 
                                                            self.num_key_value_heads], 
                                                            dim=-2)
        query_states = self.q_norm(query_states)
        key_states = self.k_norm(key_states)
        batch_meta_info = kwargs['batch_meta_info']

        q_offsets = (batch_meta_info.q_offsets if batch_meta_info.draft_offsets is None
                 else batch_meta_info.draft_offsets)
        self.rope(query_states, 
                  key_states, 
                  q_offsets, 
                  batch_meta_info.position_ids)

        attn_output = self.attention(query_states, key_states, value_states, 
                                     batch_meta_info, past_key_value)

        # model may have different hidden_size
        attn_output = attn_output.view(q_len, self.intermediate_size)  
        attn_output = self.o_proj(attn_output)

        return attn_output


class Qwen3MoeDecoderLayer(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.n_layer = config.num_hidden_layers
        self.layer_idx = layer_idx
        self.decoder_sparse_step = config.decoder_sparse_step

        # use layer_idx==None to indicate that the layer does not 
        # initialized on the current node
        if self.layer_idx is not None:
            self.self_attn = Qwen3MoeAttention(config, layer_idx=layer_idx)
            if (self.layer_idx not in config.mlp_only_layers) and (
                config.num_experts > 0 and (self.layer_idx + 1) % config.decoder_sparse_step == 0
            ):
                self.mlp = Qwen3MoeSparseMoeBlock(config, layer_idx=layer_idx) 
            else:
                self.mlp = Qwen3MoeMLP(config, layer_idx=layer_idx)
            self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
            self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx is not None and isinstance(self.mlp, Qwen3MoeMLP):
            self.mlp._flood_patch_func()

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        batch_meta_info: Optional[Batch] = None,
    ) -> torch.Tensor:

        if self.layer_idx is None:
            return hidden_states

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        hidden_states = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            batch_meta_info=batch_meta_info
        )

        hidden_states += residual

        if self.layer_idx == self.n_layer - 1 and batch_meta_info.logit_indices is not None:
            if batch_meta_info.logit_indices.numel() == 0:
                return
            hidden_states = hidden_states[batch_meta_info.logit_indices]

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)

        hidden_states += residual

        return hidden_states


class Qwen3MoeModel(PreTrainedModel):

    config_class = Qwen3MoeConfig
    base_model_prefix = "model"
    _no_split_modules = ["Qwen3MoeDecoderLayer"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.rank = int(os.environ.get('FLOOD_RANK', '0'))
        self.world_size = int(os.environ.get('FLOOD_WORLD_SIZE', '1'))

        if self.rank == 0:
            self.embed_tokens = AutoEmbedding.from_pretrained(config, 
                                                                config.vocab_size, 
                                                                config.hidden_size, 
                                                                padding_idx=self.padding_idx)
        else:
            self.embed_tokens = None 

        n_layer = config.num_hidden_layers
        layers = []
        local_size = n_layer // self.world_size
        for i in range(n_layer):
            layer_idx = i if i // local_size == self.rank else None
            layers.append(Qwen3MoeDecoderLayer(config, layer_idx=layer_idx))
        self.layers = torch.nn.ModuleList(layers)

        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value


class Qwen3MoeForCausalLM(PreTrainedModel):
    config_class = Qwen3MoeConfig
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.model = Qwen3MoeModel(config)
        self.vocab_size = config.vocab_size

        self.rank = int(os.environ.get('FLOOD_RANK', '0'))
        self.world_size = int(os.environ.get('FLOOD_WORLD_SIZE', '1'))
        if self.rank == self.world_size - 1:
            self.lm_head = AutoLinear.from_pretrained(config.hidden_size, 
                                                    config.vocab_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='lm_head')        
        else:
            self.lm_head = None
        self.sampler = Sampler()


    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model


    def flood_patch_func(self, kwargs=None):
        if hasattr(self.lm_head, 'patch'):
            print('patch lm_head')            
            self.lm_head.patch()

    @torch.inference_mode()
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        batch_meta_info : Batch = None,
        device_list : List = None,
        sync_layers : List = None,
        streams : List = None
    ) -> List:

        n_devices = len(device_list)
        for i, indices in enumerate(device_list):
            stream = streams[i]
            with torch.cuda.stream(stream):
                if i == 0 and self.rank == 0:
                    batch_meta_info.to(torch.device(0), non_blocking=True)
                    hidden_states = self.model.embed_tokens(batch_meta_info.input_ids)
                    embeddings = batch_meta_info.embeddings
                    if embeddings is not None:
                        emb_idx_list = batch_meta_info.emb_idx_list
                        for ie, emb_idx in enumerate(emb_idx_list):
                            if emb_idx is None:
                                continue
                            ss, se, ds, de = emb_idx
                            hidden_states[ds:de] = embeddings[ie][ss:se]
                sync_layers[i]()

                for j in indices:
                    hidden_states = self.model.layers[j](
                        hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        past_key_value=past_key_values,
                        batch_meta_info=batch_meta_info,
                    )

                if i < n_devices-1:
                    device = torch.device(i + 1)
                    hidden_states = hidden_states.to(device, non_blocking=True)
                    batch_meta_info.to(device, non_blocking=True)
                else:
                    if self.rank == self.world_size - 1 and hidden_states is not None:
                        hidden_states = self.model.norm(hidden_states)
                        logits = self.lm_head(hidden_states)
                        outputs = self.sampler(logits, batch_meta_info=batch_meta_info)
                    else:
                        outputs = hidden_states
                stream.synchronize()

        sync_layers[-1]()

        # TODO: adapt for multi-node serving
        if batch_meta_info.mode == 2:
            batch_meta_info.spec.update_cache(batch_meta_info.cache_src_indices, 
                                              batch_meta_info.cache_dst_indices, 
                                              past_key_values)

        return outputs
