# -*- coding: utf-8 -*-
# Copyright (c) Ant Financial Service Group and its affiliates.

# NOTE: the file is generated by scaffold.py

import os
import math
import time
import copy
from typing import List, Optional, Tuple, Union, Dict

import torch
import torch.nn.functional as F

from transformers.cache_utils import Cache
from transformers.modeling_utils import PreTrainedModel, PretrainedConfig

from flood.ops.activation import silu_and_mul
from flood.ops.norm import RMSNorm
from flood.utils.batch import Batch
from flood.layers.linear import AutoLinear
from flood.layers.rope import AutoRope
from flood.layers.attention import AutoAttention
from flood.layers.embedding import AutoEmbedding
from flood.layers.sampler import Sampler
from flood.layers.moe import AutoExperts
from .configuration_deepseekv3 import DeepseekV3Config



class DeepseekV3MLP(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, intermediate_size=None, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size
        self.intermediate_size = intermediate_size or config.intermediate_size

        self.gate_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                    self.intermediate_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='gate_proj')
        self.up_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                  self.intermediate_size, 
                                                  bias=False, 
                                                  config=config, 
                                                  name='up_proj')
            
        self.down_proj = AutoLinear.from_pretrained(self.intermediate_size, 
                                                    self.hidden_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='down_proj')

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch MLP')

        self.gate_up_proj = self.gate_proj.merge([self.gate_proj, self.up_proj])
        self.down_proj.patch()

        self.gate_proj = None
        delattr(self, 'gate_proj')

        self.up_proj = None
        delattr(self, 'up_proj')
            
    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        act = silu_and_mul(gate_up)
        return self.down_proj(act)


# class DeepseekV3MoE(torch.nn.Module):
#     def __init__(
#         self,
#         config: PretrainedConfig,
#         layer_idx: int = 0
#     ):
#         super().__init__()
#         self.config = config
#         self.layer_idx = layer_idx

#         self.n_routed_experts = config.n_routed_experts
#         self.top_k = config.num_experts_per_tok
#         self.norm_topk_prob = self.config.norm_topk_prob

#         exp_conf = copy.deepcopy(config)
#         exp_conf.intermediate_size = config.moe_intermediate_size

#         modules = torch.nn.ModuleList([DeepseekV3MLP(exp_conf, layer_idx=-1)
#                                         for _ in range(self.n_routed_experts)])
#         self.experts = AutoExperts.from_pretrained(module_list=modules,
#                                                     hidden_size=exp_conf.hidden_size,
#                                                     intermediate_size=exp_conf.intermediate_size,
#                                                     num_expert=self.n_routed_experts,
#                                                     config=config
#                                                     )

#         self.gate = torch.nn.Linear(config.hidden_size,
#                                      self.n_routed_experts,
#                                      bias=False)

#         im_sz = config.moe_intermediate_size * config.n_shared_experts
#         share_conf = copy.deepcopy(config)
#         share_conf.intermediate_size = im_sz
#         self.shared_experts = DeepseekV3MLP(config=share_conf, layer_idx=-1)

#     def flood_patch_func(self, kwargs=None):
#         self.experts._flood_patch_func()

#         self.shared_experts._flood_patch_func()

#     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
#         num_tokens, hidden_dim = hidden_states.shape
#         hidden_states = hidden_states.view(-1, hidden_dim)
#         shared_output = self.shared_experts(hidden_states)

#         router_logits = self.gate(hidden_states)

#         final_hidden_states = self.experts(hidden_states,
#                                         router_logits,
#                                         self.top_k,
#                                         renormalize=self.norm_topk_prob
#                                         )
#         final_hidden_states = final_hidden_states + shared_output

#         return final_hidden_states.view(num_tokens, hidden_dim)


class MoEGate(torch.nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok
        self.n_routed_experts = config.n_routed_experts
        self.routed_scaling_factor = config.routed_scaling_factor
        self.scoring_func = config.scoring_func
        self.topk_method = config.topk_method
        self.n_group = config.n_group
        self.topk_group = config.topk_group

        # topk selection algorithm
        self.norm_topk_prob = config.norm_topk_prob
        self.gating_dim = config.hidden_size
        self.weight = torch.nn.Parameter(
            torch.empty((self.n_routed_experts, self.gating_dim))
        )
        self.e_score_correction_bias = torch.nn.Parameter(
                torch.empty((self.n_routed_experts))
            )
        self.reset_parameters()

    def reset_parameters(self) -> None:
        import torch.nn.init as init

        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        bsz, seq_len, h = hidden_states.shape
        ### compute gating score
        hidden_states = hidden_states.view(-1, h)
        logits = F.linear(
            hidden_states.type(torch.float32), self.weight.type(torch.float32), None
        )
        scores = logits.sigmoid()


    ### select top-k experts
        scores_for_choice = scores.view(bsz * seq_len, -1) + self.e_score_correction_bias.unsqueeze(0)
        group_scores = (
            scores_for_choice.view(bsz * seq_len, self.n_group, -1).topk(2, dim=-1)[0].sum(dim = -1)
        )  # [n, n_group]
        group_idx = torch.topk(
            group_scores, k=self.topk_group, dim=-1, sorted=False
        )[
            1
        ]  # [n, top_k_group]
        group_mask = torch.zeros_like(group_scores)  # [n, n_group]
        group_mask.scatter_(1, group_idx, 1)  # [n, n_group]
        score_mask = (
            group_mask.unsqueeze(-1)
            .expand(
                bsz * seq_len, self.n_group, self.n_routed_experts // self.n_group
            )
            .reshape(bsz * seq_len, -1)
        )  # [n, e]
        tmp_scores = scores_for_choice.masked_fill(~score_mask.bool(), float("-inf"))  # [n, e]
        _, topk_idx = torch.topk(
            tmp_scores, k=self.top_k, dim=-1, sorted=False
        )
        topk_weight = scores.gather(1, topk_idx)

        ### norm gate to sum 1
        denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
        topk_weight = topk_weight / denominator
        topk_weight = topk_weight * self.routed_scaling_factor # must multiply the scaling factor

        return topk_idx, topk_weight

class DeepseekV3MoE(torch.nn.Module):
    """
    A mixed expert module containing shared experts.
    """

    def __init__(self, config, layer_idx=0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.num_experts_per_tok = config.num_experts_per_tok

        self.ep_size = 1
        self.experts_per_rank = config.n_routed_experts
        self.ep_rank = 0
        self.experts = torch.nn.ModuleList(
            [
                DeepseekV3MLP(
                    config, intermediate_size=config.moe_intermediate_size
                )
                for i in range(config.n_routed_experts)
            ]
        )
        self.gate = MoEGate(config)
        if config.n_shared_experts is not None:
            intermediate_size = config.moe_intermediate_size * config.n_shared_experts
            self.shared_experts = DeepseekV3MLP(
                config=config, intermediate_size=intermediate_size
            )

    def forward(self, hidden_states):
        identity = hidden_states
        orig_shape = hidden_states.shape
        topk_idx, topk_weight = self.gate(hidden_states)
        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])
        flat_topk_idx = topk_idx.view(-1)
        y = self.moe_infer(hidden_states, topk_idx, topk_weight).view(*orig_shape)
        y = y + self.shared_experts(identity)
        return y

    @torch.no_grad()
    def moe_infer(self, x, topk_ids, topk_weight):
        cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts)))
        cnts.scatter_(1, topk_ids, 1)
        tokens_per_expert = cnts.sum(dim=0)
        idxs = topk_ids.view(-1).argsort()
        sorted_tokens = x[idxs // topk_ids.shape[1]]
        sorted_tokens_shape = sorted_tokens.shape
        
        tokens_per_expert = tokens_per_expert.cpu().numpy()

        outputs = []
        start_idx = 0
        for i, num_tokens in enumerate(tokens_per_expert):
            end_idx = start_idx + num_tokens
            if num_tokens == 0:
                continue
            expert = self.experts[i + self.ep_rank * self.experts_per_rank]
            tokens_for_this_expert = sorted_tokens[start_idx:end_idx]
            expert_out = expert(tokens_for_this_expert)
            outputs.append(expert_out)
            start_idx = end_idx

        outs = torch.cat(outputs, dim=0) if len(outputs) else sorted_tokens.new_empty(0)

        new_x = torch.empty_like(outs)
        new_x[idxs] = outs
        final_out = (
            new_x.view(*topk_ids.shape, -1)
            .type(topk_weight.dtype)
            .mul_(topk_weight.unsqueeze(dim=-1))
            .sum(dim=1)
            .type(new_x.dtype)
        )
        return final_out


class DeepseekV3Attention(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads

        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.q_lora_rank = config.q_lora_rank
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.kv_lora_rank = config.kv_lora_rank
        self.v_head_dim = config.v_head_dim
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim


        self.q_a_proj = AutoLinear.from_pretrained(
                self.hidden_size, config.q_lora_rank, bias=config.attention_bias, config=config
            )
        self.q_a_layernorm = RMSNorm(config.q_lora_rank, eps=1e-6)
        self.q_b_proj = AutoLinear.from_pretrained(
            config.q_lora_rank, self.num_heads * self.q_head_dim, bias=False, config=config
        )


        self.kv_a_proj_with_mqa = AutoLinear.from_pretrained(
            self.hidden_size,
            config.kv_lora_rank + config.qk_rope_head_dim,
            bias=config.attention_bias,
            config=config
        )
        self.kv_a_layernorm = RMSNorm(config.kv_lora_rank, eps=1e-6)
        self.kv_b_proj = AutoLinear.from_pretrained(
            config.kv_lora_rank,
            self.num_heads * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
            bias=False, 
            config=config
        )

        self.o_proj = AutoLinear.from_pretrained(
            self.num_heads * self.v_head_dim,
            self.hidden_size,
            bias=config.attention_bias, 
            config=config
        )

        self.softmax_scale = self.q_head_dim ** (-0.5)

        self.rope = AutoRope.from_pretrained(config)
        self.attention =  None

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch Attention')

        if kwargs is None:
            kwargs = {}
        cache_dtype = kwargs.get('cache_dtype', None)
        interleave_value = kwargs.get('interleave_value', False)
        if interleave_value:
            AutoAttention.interleave(self.qkv_proj, 
                                     self.num_heads, 
                                     self.num_key_value_heads, 
                                     self.head_dim)

        kernels = kwargs.get('kernels', ['sa'])
        self.attention = AutoAttention.from_pretrained(cache_dtype, 
                                                       layer_idx=self.layer_idx, 
                                                       kernels=kernels,
                                                       softmax_scale=self.softmax_scale)


    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        **kwargs,
    ) -> torch.Tensor:

        q_len = hidden_states.size(0)
        batch_meta_info = kwargs['batch_meta_info']

        q = self.q_b_proj(self.q_a_layernorm(self.q_a_proj(hidden_states)))
        q = q.view(q_len, self.num_heads, self.q_head_dim)
        q_nope, q_pe = torch.split(
            q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
        )
        qa = torch.empty((q_len, self.num_heads, self.kv_lora_rank+self.qk_rope_head_dim), device=q.device, dtype=q.dtype)

        kv = self.kv_a_proj_with_mqa(hidden_states)
        kv[:,:self.kv_lora_rank] = self.kv_a_layernorm(kv[:,:self.kv_lora_rank])
        self.rope(q_pe, kv[:,None,self.kv_lora_rank:], batch_meta_info.q_offsets, batch_meta_info.pids)
        qa[:,:,self.kv_lora_rank:] = q_pe

        w = self.kv_b_proj.weight.data.view(128,2,128, self.kv_lora_rank).to(q_nope.dtype)
        w0 = w[:,0]
        w1 = w[:,1]
        # q_nope: [q_len, head_num, dim]
        # w0: [head_num, dim, lora]
        qa[:,:,:self.kv_lora_rank] = torch.einsum("lhd,hdr->lhr", q_nope, w0)

        # attn_output:[q_len,head_num,lora]
        attn_output = self.attention(qa, kv, 
                                     batch_meta_info, past_key_value)
        # attn_output:[q_len,head_num,dim]
        attn_output = torch.einsum("lhr,hdr->lhd", attn_output, w1)

        # model may have different hidden_size
        attn_output = attn_output.reshape(q_len, self.num_heads*self.v_head_dim)  
        attn_output = self.o_proj(attn_output)

        return attn_output


class DeepseekV3DecoderLayer(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx

        # use layer_idx==None to indicate that the layer does not 
        # initialized on the current node, and use layer_idx==-1
        # to indicate the final layer of the model
        if self.layer_idx is not None:
            self.self_attn = DeepseekV3Attention(config, layer_idx=layer_idx)
            self.mlp = DeepseekV3MoE(config, layer_idx=layer_idx) if self.layer_idx >= config.first_k_dense_replace else DeepseekV3MLP(config, layer_idx=layer_idx)
            self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
            self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        batch_meta_info: Optional[Batch] = None,
    ) -> torch.Tensor:

        if self.layer_idx is None:
            return hidden_states

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        hidden_states = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            batch_meta_info=batch_meta_info
        )

        hidden_states += residual

        if self.layer_idx == -1 and batch_meta_info.logit_indices is not None:
            if batch_meta_info.logit_indices.numel() == 0:
                return
            hidden_states = hidden_states[batch_meta_info.logit_indices]

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)

        hidden_states += residual

        return hidden_states


class DeepseekV3Model(PreTrainedModel):

    config_class = DeepseekV3Config
    base_model_prefix = "model"
    _no_split_modules = ["DeepseekV3DecoderLayer"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))

        if rank == 0:
            self.embed_tokens = AutoEmbedding.from_pretrained(config, 
                                                                config.vocab_size, 
                                                                config.hidden_size, 
                                                                padding_idx=self.padding_idx)
        else:
            self.embed_tokens = None 

        n_layer = config.num_hidden_layers
        layers = []
        local_size = n_layer // world_size
        for i in range(n_layer):
            layer_idx = i if i // local_size == rank else None
            layer_idx = -1 if layer_idx == n_layer - 1 and rank == world_size - 1 else layer_idx
            layers.append(DeepseekV3DecoderLayer(config, layer_idx=layer_idx))
        self.layers = torch.nn.ModuleList(layers)

        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value


class DeepseekV3ForCausalLM(PreTrainedModel):
    config_class = DeepseekV3Config
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.model = DeepseekV3Model(config)
        self.vocab_size = config.vocab_size

        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))
        if rank == world_size - 1:
            self.lm_head = AutoLinear.from_pretrained(config.hidden_size, 
                                                    config.vocab_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='lm_head')        
        else:
            self.lm_head = None
        self.sampler = Sampler()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def flood_patch_func(self, kwargs=None):
        if hasattr(self.lm_head, 'patch'):
            print('patch lm_head')            
            self.lm_head.patch()


    @torch.inference_mode()
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        batch_meta_info : Batch = None,
        device_list : List = None,
        sync_layers : List = None,
        streams : List = None
    ) -> List:

        n_devices = len(device_list)
        n_layers = len(self.model.layers)
        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))
        for i, indices in enumerate(device_list):
            stream = streams[i]
            with torch.cuda.stream(stream):
                if i == 0 and rank == 0:
                    batch_meta_info.to(torch.device(0), non_blocking=True)
                    hidden_states = self.model.embed_tokens(batch_meta_info.input_ids)
                    embeddings = batch_meta_info.embeddings
                    if embeddings is not None:
                        emb_idx_list = batch_meta_info.emb_idx_list
                        for ie, emb_idx in enumerate(emb_idx_list):
                            if emb_idx is None:
                                continue
                            ss, se, ds, de = emb_idx
                            hidden_states[ds:de] = embeddings[ie][ss:se]
                sync_layers[i]()

                for j in indices:
                    hidden_states = self.model.layers[j](
                        hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        past_key_value=past_key_values,
                        batch_meta_info=batch_meta_info,
                    )

                if i < n_devices-1:
                    device = torch.device(i + 1)
                    hidden_states = hidden_states.to(device, non_blocking=True)
                    batch_meta_info.to(device, non_blocking=True)
                else:
                    if rank == world_size - 1 and hidden_states is not None:
                        hidden_states = self.model.norm(hidden_states)
                        logits = self.lm_head(hidden_states)
                        outputs = self.sampler(logits, batch_meta_info=batch_meta_info)
                    else:
                        outputs = hidden_states
                stream.synchronize()

        sync_layers[-1]()

        return outputs
