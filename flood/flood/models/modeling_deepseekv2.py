# -*- coding: utf-8 -*-
# Copyright (c) Ant Financial Service Group and its affiliates.

# NOTE: the file is generated by scaffold.py

import os
import math
import time
import copy
from typing import List, Optional, Tuple, Union, Dict

import torch
import torch.nn.functional as F

from transformers.cache_utils import Cache
from transformers.modeling_utils import PreTrainedModel, PretrainedConfig

from flood.ops.activation import silu_and_mul
from flood.ops.norm import RMSNorm
from flood.utils.batch import Batch
from flood.layers.linear import AutoLinear
from flood.layers.rope import AutoRope
from flood.layers.attention import AutoAttention
from flood.layers.embedding import AutoEmbedding
from flood.layers.sampler import Sampler
from flood.layers.moe import AutoExperts
from transformers.models.deepseek_v2.configuration_deepseek_v2 import DeepseekV2Config


class DeepseekV2MLP(torch.nn.Module):
    def __init__(
        self, config: PretrainedConfig, intermediate_size=None, layer_idx: int = 0
    ):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size
        self.intermediate_size = intermediate_size or config.intermediate_size

        self.gate_proj = AutoLinear.from_pretrained(
            self.hidden_size,
            self.intermediate_size,
            bias=False,
            config=config,
            name="gate_proj",
        )
        self.up_proj = AutoLinear.from_pretrained(
            self.hidden_size,
            self.intermediate_size,
            bias=False,
            config=config,
            name="up_proj",
        )

        self.down_proj = AutoLinear.from_pretrained(
            self.intermediate_size,
            self.hidden_size,
            bias=False,
            config=config,
            name="down_proj",
        )

    def _flood_patch_func(self, kwargs=None):
        # if self.layer_idx == 0:
        #     print('patch MLP')

        self.gate_up_proj = self.gate_proj.merge([self.gate_proj, self.up_proj])
        self.down_proj.patch()

        self.gate_proj = None
        delattr(self, "gate_proj")

        self.up_proj = None
        delattr(self, "up_proj")

    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        act = silu_and_mul(gate_up)
        return self.down_proj(act)


class DeepseekV2MoE(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.n_routed_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.norm_topk_prob = self.config.norm_topk_prob
        self.routed_scaling_factor = config.routed_scaling_factor
        self.topk_group = config.topk_group
        self.num_expert_group = config.n_group

        exp_conf = copy.deepcopy(config)
        exp_conf.intermediate_size = config.moe_intermediate_size

        modules = torch.nn.ModuleList(
            [
                DeepseekV2MLP(exp_conf, layer_idx=-1)
                for _ in range(self.n_routed_experts)
            ]
        )
        self.experts = AutoExperts.from_pretrained(
            module_list=modules,
            hidden_size=exp_conf.hidden_size,
            intermediate_size=exp_conf.intermediate_size,
            num_expert=self.n_routed_experts,
            scoring_func="softmax",
            num_expert_group=self.num_expert_group,
            topk_group=self.topk_group,
            config=config,
        )

        self.gate = AutoLinear.from_pretrained(
            config.hidden_size,
            self.n_routed_experts,
            bias=False,
            config=config,
            name="gate",
        )

        im_sz = config.moe_intermediate_size * config.n_shared_experts
        share_conf = copy.deepcopy(config)
        share_conf.intermediate_size = im_sz
        self.shared_experts = DeepseekV2MLP(config=share_conf, layer_idx=-1)

    def flood_patch_func(self, kwargs=None):
        self.experts._flood_patch_func()

        self.shared_experts._flood_patch_func()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        num_tokens, hidden_dim = hidden_states.shape
        hidden_states = hidden_states.view(-1, hidden_dim)
        shared_output = self.shared_experts(hidden_states)

        router_logits = self.gate(hidden_states)

        final_hidden_states = self.experts(
            hidden_states,
            router_logits,
            self.top_k,
            renormalize=self.norm_topk_prob,
        )
        final_hidden_states = (
            final_hidden_states * self.routed_scaling_factor + shared_output
        )

        return final_hidden_states.view(num_tokens, hidden_dim)


class DeepseekV2Attention(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads

        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = config.rope_theta
        self.q_lora_rank = config.q_lora_rank
        self.qk_rope_head_dim = config.qk_rope_head_dim
        self.kv_lora_rank = config.kv_lora_rank
        self.v_head_dim = config.v_head_dim
        self.qk_nope_head_dim = config.qk_nope_head_dim
        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim

        self.q_proj = AutoLinear.from_pretrained(
            self.hidden_size,
            self.num_heads * self.q_head_dim,
            bias=config.attention_bias,
            config=config,
        )

        self.kv_a_proj_with_mqa = AutoLinear.from_pretrained(
            self.hidden_size,
            config.kv_lora_rank + config.qk_rope_head_dim,
            bias=config.attention_bias,
            config=config,
        )
        self.kv_a_layernorm = RMSNorm(config.kv_lora_rank, eps=1e-6)
        self.kv_b_proj = AutoLinear.from_pretrained(
            config.kv_lora_rank,
            self.num_heads
            * (self.q_head_dim - self.qk_rope_head_dim + self.v_head_dim),
            bias=False,
            config=config,
        )

        self.o_proj = AutoLinear.from_pretrained(
            self.num_heads * self.v_head_dim,
            self.hidden_size,
            bias=config.attention_bias,
            config=config,
        )

        self.softmax_scale = self.q_head_dim ** (-0.5)

        self.rope = AutoRope.from_pretrained(config)

        self.attention = None

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print("patch Attention")

        if kwargs is None:
            kwargs = {}
        cache_dtype = kwargs.get("cache_dtype", None)
        interleave_value = kwargs.get("interleave_value", False)
        if interleave_value:
            AutoAttention.interleave(
                self.qkv_proj, self.num_heads, self.num_key_value_heads, self.head_dim
            )

        kernels = kwargs.get("kernels", ["sa"])
        self.attention = AutoAttention.from_pretrained(
            cache_dtype,
            layer_idx=self.layer_idx,
            kernels=kernels,
            softmax_scale=self.softmax_scale,
        )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        **kwargs,
    ) -> torch.Tensor:

        q_len = hidden_states.size(0)
        batch_meta_info = kwargs["batch_meta_info"]

        q = self.q_proj(hidden_states)
        q = q.view(q_len, self.num_heads, self.q_head_dim)
        q_nope, q_pe = torch.split(
            q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1
        )
        qa = torch.empty(
            (q_len, self.num_heads, self.kv_lora_rank + self.qk_rope_head_dim),
            device=q.device,
            dtype=q.dtype,
        )

        kv = self.kv_a_proj_with_mqa(hidden_states)
        kv[:, : self.kv_lora_rank] = self.kv_a_layernorm(kv[:, : self.kv_lora_rank])
        self.rope(
            q_pe,
            kv[:, None, self.kv_lora_rank :],
            batch_meta_info.q_offsets,
            batch_meta_info.position_ids,
        )
        qa[:, :, self.kv_lora_rank :] = q_pe

        w = self.kv_b_proj.weight.data.view(
            self.num_heads, 2, 128, self.kv_lora_rank
        ).to(q_nope.dtype)
        w0 = w[:, 0]
        w1 = w[:, 1]
        # q_nope: [q_len, head_num, dim]
        # w0: [head_num, dim, lora]
        qa[:, :, : self.kv_lora_rank] = torch.einsum("lhd,hdr->lhr", q_nope, w0)

        # attn_output:[q_len,head_num,lora]
        attn_output = self.attention(qa, kv, batch_meta_info, past_key_value)
        # attn_output:[q_len,head_num,dim]
        attn_output = torch.einsum("lhr,hdr->lhd", attn_output, w1)

        # model may have different hidden_size
        attn_output = attn_output.reshape(q_len, self.num_heads * self.v_head_dim)
        attn_output = self.o_proj(attn_output)

        return attn_output


class DeepseekV2DecoderLayer(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx
        self.n_layer = config.num_hidden_layers

        # use layer_idx==None to indicate that the layer does not
        # initialized on the current node, and use layer_idx==-1
        # to indicate the final layer of the model
        if self.layer_idx is not None:
            self.self_attn = DeepseekV2Attention(config, layer_idx=layer_idx)
            self.mlp = (
                DeepseekV2MoE(config, layer_idx=layer_idx)
                if self.layer_idx >= config.first_k_dense_replace
                else DeepseekV2MLP(config, layer_idx=layer_idx)
            )
            self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
            self.post_attention_layernorm = RMSNorm(
                config.hidden_size, eps=config.rms_norm_eps
            )

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        batch_meta_info: Optional[Batch] = None,
    ) -> torch.Tensor:

        if self.layer_idx is None:
            return hidden_states

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        hidden_states = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            batch_meta_info=batch_meta_info,
        )

        hidden_states += residual

        if (
            self.layer_idx == self.n_layer - 1
            and batch_meta_info.logit_indices is not None
        ):
            if batch_meta_info.logit_indices.numel() == 0:
                return
            hidden_states = hidden_states[batch_meta_info.logit_indices]

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)

        hidden_states += residual

        return hidden_states

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx is not None and isinstance(self.mlp, DeepseekV2MLP):
            self.mlp._flood_patch_func()


class DeepseekV2Model(PreTrainedModel):

    config_class = DeepseekV2Config
    base_model_prefix = "model"
    _no_split_modules = ["DeepseekV2DecoderLayer"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.rank = int(os.environ.get("FLOOD_RANK", "0"))
        self.world_size = int(os.environ.get("FLOOD_WORLD_SIZE", "1"))

        if self.rank == 0:
            self.embed_tokens = AutoEmbedding.from_pretrained(
                config,
                config.vocab_size,
                config.hidden_size,
                padding_idx=self.padding_idx,
            )
        else:
            self.embed_tokens = None

        n_layer = config.num_hidden_layers
        layers = []
        local_size = (n_layer - 1) // self.world_size + 1
        for i in range(n_layer):
            layer_idx = i if i // local_size == self.rank else None
            layers.append(DeepseekV2DecoderLayer(config, layer_idx=layer_idx))
        self.layers = torch.nn.ModuleList(layers)

        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value


class DeepseekV2ForCausalLM(PreTrainedModel):
    config_class = DeepseekV2Config
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.model = DeepseekV2Model(config)
        self.vocab_size = config.vocab_size

        self.rank = int(os.environ.get("FLOOD_RANK", "0"))
        self.world_size = int(os.environ.get("FLOOD_WORLD_SIZE", "1"))
        if self.rank == self.world_size - 1:
            self.lm_head = AutoLinear.from_pretrained(
                config.hidden_size,
                config.vocab_size,
                bias=False,
                config=config,
                name="lm_head",
            )
        else:
            self.lm_head = None
        self.sampler = Sampler()

    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model

    def flood_patch_func(self, kwargs=None):
        if hasattr(self.lm_head, "patch"):
            print("patch lm_head")
            self.lm_head.patch()

    @torch.inference_mode()
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        batch_meta_info: Batch = None,
        device_list: List = None,
        sync_layers: List = None,
        streams: List = None,
    ) -> List:

        n_devices = len(device_list)
        n_layers = len(self.model.layers)
        for i, indices in enumerate(device_list):
            stream = streams[i]
            with torch.cuda.stream(stream):
                if i == 0 and self.rank == 0:
                    batch_meta_info.to(torch.device(0), non_blocking=True)
                    hidden_states = self.model.embed_tokens(batch_meta_info.input_ids)
                    embeddings = batch_meta_info.embeddings
                    if embeddings is not None:
                        emb_idx_list = batch_meta_info.emb_idx_list
                        for ie, emb_idx in enumerate(emb_idx_list):
                            if emb_idx is None:
                                continue
                            ss, se, ds, de = emb_idx
                            hidden_states[ds:de] = embeddings[ie][ss:se]
                sync_layers[i]()

                for j in indices:
                    hidden_states = self.model.layers[j](
                        hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        past_key_value=past_key_values,
                        batch_meta_info=batch_meta_info,
                    )
                if i < n_devices - 1:
                    device = torch.device(i + 1)
                    hidden_states = hidden_states.to(device, non_blocking=True)
                    batch_meta_info.to(device, non_blocking=True)
                else:
                    if self.rank == self.world_size - 1 and hidden_states is not None:
                        hidden_states = self.model.norm(hidden_states)
                        logits = self.lm_head(hidden_states)
                        outputs = self.sampler(logits, batch_meta_info=batch_meta_info)
                    else:
                        outputs = hidden_states
                stream.synchronize()
        sync_layers[-1]()

        return outputs
