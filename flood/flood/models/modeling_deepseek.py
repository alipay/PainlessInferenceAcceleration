# -*- coding: utf-8 -*-
# Copyright (c) Ant Financial Service Group and its affiliates.

# NOTE: the file is generated by scaffold.py

import os
import math
import time
import copy
from typing import List, Optional, Tuple, Union, Dict

import torch
from transformers.cache_utils import Cache
from transformers.modeling_utils import PreTrainedModel, PretrainedConfig

from flood.ops import RMSNorm, silu_and_mul
from flood.utils.batch import Batch
from flood.layers.linear import AutoLinear
from flood.layers.rope import AutoRope
from flood.layers.attention import AutoAttention
from flood.layers.embedding import AutoEmbedding
from flood.layers.sampler import Sampler
from flood.layers.moe import AutoExperts
from .configuration_deepseek import DeepseekConfig



class DeepseekMLP(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size

        self.gate_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                    self.intermediate_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='gate_proj')
        self.up_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                  self.intermediate_size, 
                                                  bias=False, 
                                                  config=config, 
                                                  name='up_proj')
            
        self.down_proj = AutoLinear.from_pretrained(self.intermediate_size, 
                                                    self.hidden_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='down_proj')

    def _flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch MLP')

        self.gate_up_proj = self.gate_proj.merge([self.gate_proj, self.up_proj])
        self.down_proj.patch()

        self.gate_proj = None
        delattr(self, 'gate_proj')

        self.up_proj = None
        delattr(self, 'up_proj')
            
    def forward(self, x):
        gate_up = self.gate_up_proj(x)
        act = silu_and_mul(gate_up)
        return self.down_proj(act)


class DeepseekMoE(torch.nn.Module):
    def __init__(
        self,
        config: PretrainedConfig,
        layer_idx: int = 0
    ):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.n_routed_experts = config.n_routed_experts
        self.top_k = config.num_experts_per_tok
        self.norm_topk_prob = self.config.norm_topk_prob

        exp_conf = copy.deepcopy(config)
        exp_conf.intermediate_size = config.moe_intermediate_size

        modules = torch.nn.ModuleList([DeepseekMLP(exp_conf, layer_idx=-1)
                                        for _ in range(self.n_routed_experts)])
        self.experts = AutoExperts.from_pretrained(module_list=modules,
                                                    hidden_size=exp_conf.hidden_size,
                                                    intermediate_size=exp_conf.intermediate_size,
                                                    num_expert=self.n_routed_experts,
                                                    config=config
                                                    )

        self.gate = torch.nn.Linear(config.hidden_size,
                                     self.n_routed_experts,
                                     bias=False)

        im_sz = config.moe_intermediate_size * config.n_shared_experts
        share_conf = copy.deepcopy(config)
        share_conf.intermediate_size = im_sz
        self.shared_experts = DeepseekMLP(config=share_conf, layer_idx=-1)

    def flood_patch_func(self, kwargs=None):
        self.experts._flood_patch_func()

        self.shared_experts._flood_patch_func()

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        num_tokens, hidden_dim = hidden_states.shape
        hidden_states = hidden_states.view(-1, hidden_dim)
        shared_output = self.shared_experts(hidden_states)

        router_logits = self.gate(hidden_states)

        final_hidden_states = self.experts(hidden_states,
                                        router_logits,
                                        self.top_k,
                                        renormalize=self.norm_topk_prob
                                        )
        final_hidden_states = final_hidden_states + shared_output

        return final_hidden_states.view(num_tokens, hidden_dim)


class DeepseekAttention(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx

        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        head_dim = None
        if hasattr(config, 'head_dim'):
            head_dim = config.head_dim
        if head_dim is None or head_dim<=0:
            head_dim = self.hidden_size // self.num_heads
        self.head_dim = head_dim
        self.intermediate_size = self.num_heads * self.head_dim
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.rope_theta = float(config.rope_theta)
        self.softmax_scale = math.sqrt(1.0 / self.head_dim)

        self.q_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_heads * self.head_dim, 
                                                 bias=False, 
                                                 config=config, 
                                                 name='q_proj')
        self.k_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_key_value_heads * self.head_dim, 
                                                 bias=False, 
                                                 config=config, 
                                                 name='k_proj')
        self.v_proj = AutoLinear.from_pretrained(self.hidden_size, 
                                                 self.num_key_value_heads * self.head_dim, 
                                                 bias=False, 
                                                 config=config, 
                                                 name='v_proj')
            
        self.o_proj = AutoLinear.from_pretrained(self.intermediate_size, 
                                                 self.hidden_size, 
                                                 bias=False, 
                                                 config=config, 
                                                 name='o_proj')

        self.rope = AutoRope.from_pretrained(config)
        self.attention =  None

    def flood_patch_func(self, kwargs=None):
        if self.layer_idx == 0:
            print('patch Attention')


        self.qkv_proj = self.q_proj.merge([self.q_proj, self.k_proj, self.v_proj])

        self.o_proj.patch()

        self.q_proj = None
        delattr(self, 'q_proj')

        self.k_proj = None
        delattr(self, 'k_proj')

        self.v_proj = None
        delattr(self, 'v_proj')

        if kwargs is None:
            kwargs = {}
        cache_dtype = kwargs.get('cache_dtype', None)
        interleave_value = kwargs.get('interleave_value', False)
        if interleave_value:
            AutoAttention.interleave(self.qkv_proj, 
                                     self.num_heads, 
                                     self.num_key_value_heads, 
                                     self.head_dim)

        kernels = kwargs.get('kernels', ['sa'])
        self.attention = AutoAttention.from_pretrained(cache_dtype, 
                                                       layer_idx=self.layer_idx, 
                                                       kernels=kernels,
                                                       softmax_scale=self.softmax_scale)


    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        **kwargs,
    ) -> torch.Tensor:

        q_len = hidden_states.size(0)
        qkv = self.qkv_proj(hidden_states)
        qkv = qkv.view(q_len, self.num_heads + 2 * self.num_key_value_heads, self.head_dim)

        query_states, key_states, value_states = qkv.split([self.num_heads, 
                                                            self.num_key_value_heads, 
                                                            self.num_key_value_heads], 
                                                            dim=-2)

        batch_meta_info = kwargs['batch_meta_info']

        self.rope(query_states, key_states, batch_meta_info.q_offsets, batch_meta_info.pids)

        attn_output = self.attention(query_states, key_states, value_states, 
                                     batch_meta_info, past_key_value)

        # model may have different hidden_size
        attn_output = attn_output.view(q_len, self.intermediate_size)  
        attn_output = self.o_proj(attn_output)

        return attn_output


class DeepseekDecoderLayer(torch.nn.Module):
    def __init__(self, config: PretrainedConfig, layer_idx: int = 0):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.layer_idx = layer_idx

        # use layer_idx==None to indicate that the layer does not 
        # initialized on the current node, and use layer_idx==-1
        # to indicate the final layer of the model
        if self.layer_idx is not None:
            self.self_attn = DeepseekAttention(config, layer_idx=layer_idx)
            self.mlp = DeepseekMoE(config, layer_idx=layer_idx)
            self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
            self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        batch_meta_info: Optional[Batch] = None,
    ) -> torch.Tensor:

        if self.layer_idx is None:
            return hidden_states

        residual = hidden_states

        hidden_states = self.input_layernorm(hidden_states)

        hidden_states = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            batch_meta_info=batch_meta_info
        )

        hidden_states += residual

        if self.layer_idx == -1 and batch_meta_info.logit_indices is not None:
            if batch_meta_info.logit_indices.numel() == 0:
                return
            hidden_states = hidden_states[batch_meta_info.logit_indices]

        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)

        hidden_states += residual

        return hidden_states


class DeepseekModel(PreTrainedModel):

    config_class = DeepseekConfig
    base_model_prefix = "model"
    _no_split_modules = ["DeepseekDecoderLayer"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))

        if rank == 0:
            self.embed_tokens = AutoEmbedding.from_pretrained(config, 
                                                                config.vocab_size, 
                                                                config.hidden_size, 
                                                                padding_idx=self.padding_idx)
        else:
            self.embed_tokens = None 

        n_layer = config.num_hidden_layers
        layers = []
        local_size = n_layer // world_size
        for i in range(n_layer):
            layer_idx = i if i // local_size == rank else None
            layer_idx = -1 if layer_idx == n_layer - 1 and rank == world_size - 1 else layer_idx
            layers.append(DeepseekDecoderLayer(config, layer_idx=layer_idx))
        self.layers = torch.nn.ModuleList(layers)

        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

    def get_input_embeddings(self):
        return self.embed_tokens

    def set_input_embeddings(self, value):
        self.embed_tokens = value


class DeepseekForCausalLM(PreTrainedModel):
    config_class = DeepseekConfig
    _tied_weights_keys = ["lm_head.weight"]

    def __init__(self, config: PretrainedConfig):
        super().__init__(config)
        self.model = DeepseekModel(config)
        self.vocab_size = config.vocab_size

        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))
        if rank == world_size - 1:
            self.lm_head = AutoLinear.from_pretrained(config.hidden_size, 
                                                    config.vocab_size, 
                                                    bias=False, 
                                                    config=config, 
                                                    name='lm_head')        
        else:
            self.lm_head = None
        self.sampler = Sampler()


    def get_input_embeddings(self):
        return self.model.embed_tokens

    def set_input_embeddings(self, value):
        self.model.embed_tokens = value

    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def set_decoder(self, decoder):
        self.model = decoder

    def get_decoder(self):
        return self.model


    def flood_patch_func(self, kwargs=None):
        if hasattr(self.lm_head, 'patch'):
            print('patch lm_head')            
            self.lm_head.patch()


    @torch.inference_mode()
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        hidden_states: Optional[torch.FloatTensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Cache] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        batch_meta_info : Batch = None,
        device_list : List = None,
        sync_layers : List = None,
        streams : List = None
    ) -> List:

        n_devices = len(device_list)
        n_layers = len(self.model.layers)
        rank = int(os.environ.get('RANK', '0'))
        world_size = int(os.environ.get('WORLD_SIZE', '1'))
        for i, indices in enumerate(device_list):
            stream = streams[i]
            with torch.cuda.stream(stream):
                if i == 0 and rank == 0:
                    batch_meta_info.to(torch.device(0), non_blocking=True)
                    hidden_states = self.model.embed_tokens(batch_meta_info.input_ids)
                    embeddings = batch_meta_info.embeddings
                    if embeddings is not None:
                        emb_idx_list = batch_meta_info.emb_idx_list
                        for ie, emb_idx in enumerate(emb_idx_list):
                            if emb_idx is None:
                                continue
                            ss, se, ds, de = emb_idx
                            hidden_states[ds:de] = embeddings[ie][ss:se]
                sync_layers[i]()

                for j in indices:
                    hidden_states = self.model.layers[j](
                        hidden_states,
                        attention_mask=attention_mask,
                        position_ids=position_ids,
                        past_key_value=past_key_values,
                        batch_meta_info=batch_meta_info,
                    )

                if i < n_devices-1:
                    device = torch.device(i + 1)
                    hidden_states = hidden_states.to(device, non_blocking=True)
                    batch_meta_info.to(device, non_blocking=True)
                else:
                    if rank == world_size - 1 and hidden_states is not None:
                        hidden_states = self.model.norm(hidden_states)
                        logits = self.lm_head(hidden_states)
                        outputs = self.sampler(logits, batch_meta_info=batch_meta_info)
                    else:
                        outputs = hidden_states
                stream.synchronize()

        sync_layers[-1]()

        return outputs
